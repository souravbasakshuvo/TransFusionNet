{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Core Libraries\nimport os\nimport glob\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\n# Image Processing\nfrom skimage.filters import sobel\n\n# TensorFlow / Keras\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Model, regularizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Common Layers\nfrom tensorflow.keras.layers import (\n    Dense, Dropout, Flatten, Input,\n    Conv2D, SeparableConv2D, Conv2DTranspose,\n    Concatenate, Add, Activation,\n    GlobalAveragePooling2D, GlobalMaxPooling2D,\n    MaxPooling2D, AveragePooling2D, BatchNormalization\n)\n\n# Pretrained Models\nfrom tensorflow.keras.applications import (\n    VGG16, VGG19, ResNet50, ResNet101,\n    EfficientNetB0, EfficientNetB2, EfficientNetB3,\n    EfficientNetB4, EfficientNetB5, EfficientNetB6,\n    InceptionResNetV2, MobileNetV2\n)\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# Visualization\nimport visualkeras\n","metadata":{"_cell_guid":"61777543-722e-4ed4-b5e6-7d23b412f76e","_uuid":"278b1c63-81b5-4675-b607-9b8ccf9d4dd4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"####  Transition Layer  ######\ndef transition_block(inputs):\n    x = tf.keras.layers.SeparableConv2D(filters=64, kernel_size=1)(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.SeparableConv2D(filters=128, kernel_size=3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=1)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    #x = tf.keras.layers.UpSampling2D((2,2))(x)\n    x = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n    \n\n   \n    return x\n\ndef additionalL(inputs):\n    x = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=1)(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    return x","metadata":{"_cell_guid":"93427d7e-4b50-428a-8c66-e5125ec08413","_uuid":"7eea9f39-6d54-490d-a5ae-c2db888397b3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef transition_layer(inputs):\n    x = Conv2D(512, (1, 1), padding='valid', activation='relu')(inputs)\n    while x.shape[1] > 4:  # Adjust to your desired output size\n        x = Conv2D(512, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n    return x","metadata":{"_cell_guid":"bde9d5b1-6f59-406e-aacc-05915d34efac","_uuid":"43e60988-466b-476e-84dd-3b728dcd80c2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def VM(input_size = (128,128,3),input = input):\n    vision_model = vit.vit_b16(input_size[0])(input)\n    M= tf.keras.Model(inputs=input, outputs=vision_model)\n    return M","metadata":{"_cell_guid":"f05271bb-b89a-4cb4-9dc0-2ca77c3fa555","_uuid":"d0feec23-b5fe-4466-977b-d88ff2810a5a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16, ResNet50, VGG19,ResNet101\nfrom vit_keras import vit, utils","metadata":{"_cell_guid":"af996081-24db-429b-adc6-985f498d1385","_uuid":"c7809c88-4cfa-4ad9-b3be-8c00e8adb646","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model(input_size=(128, 128, 3)):\n    input_tensor = Input(shape=input_size)\n\n    # Load VGG19 model without top layers and freeze its weights\n    base_model1 = VGG19(include_top=False, weights='imagenet', pooling='max',\n                        input_shape=input_size, input_tensor=input_tensor)\n    for layer in base_model1.layers:\n        layer.trainable = False\n\n    # Load ResNet50 model without top layers and freeze its weights\n    base_model2 = ResNet50(include_top=False, weights='imagenet', pooling='max',\n                           input_shape=input_size, input_tensor=input_tensor)\n    for layer in base_model2.layers:\n        layer.trainable = False\n\n    # Load Vision Transformer output\n    vision_output = VM(input_size, input_tensor)\n\n    # Extract intermediate layers from VGG19 for bi-fusion and feature fusion\n    last_layer11 = base_model1.get_layer('block3_conv2').output\n    last_layer12 = base_model1.get_layer('block4_conv3').output\n    last_layer13 = base_model1.get_layer('block5_conv1').output\n    last_layer14 = base_model1.get_layer('block5_conv3').output\n    last_layer15 = base_model1.get_layer('block5_pool').output\n\n    # Extract intermediate layers from ResNet50 for bi-fusion and feature fusion\n    last_layer21 = base_model2.get_layer('conv2_block1_out').output\n    last_layer22 = base_model2.get_layer('conv3_block4_out').output\n    last_layer23 = base_model2.get_layer('conv4_block4_out').output\n    last_layer24 = base_model2.get_layer('conv4_block6_out').output\n    last_layer25 = base_model2.get_layer('conv5_block3_out').output\n\n    # Apply transition block 2 to VGG19 feature maps to get uniform 4x4x256 shape\n    vgg1 = transition_layer(last_layer11)\n    vgg2 = transition_layer(last_layer12)\n    vgg3 = transition_layer(last_layer13)\n    vgg4 = transition_layer(last_layer14)\n\n    # Apply transition block 2 to ResNet50 feature maps to get uniform 4x4x256 shape\n    resnet1 = transition_layer(last_layer21)\n    resnet2 = transition_layer(last_layer22)\n    resnet3 = transition_layer(last_layer23)\n    resnet4 = transition_layer(last_layer24)\n\n    # Bi-fusion of corresponding VGG19 and ResNet50 features\n    merge1 = tf.keras.layers.Concatenate(axis=-1)([vgg1, resnet1])\n    merge2 = tf.keras.layers.Concatenate(axis=-1)([vgg2, resnet2])\n    merge3 = tf.keras.layers.Concatenate(axis=-1)([vgg3, resnet3])\n    merge4 = tf.keras.layers.Concatenate(axis=-1)([vgg4, resnet4])\n\n    # Feature fusion: combine merged features to form left and right feature streams\n    outputL = tf.keras.layers.Concatenate(axis=-1)([merge1, merge3])\n    outputR = tf.keras.layers.Concatenate(axis=-1)([merge2, merge4])\n\n    # Passing left fused features through Transition Block 1\n    outputLT = transition_block(outputL)\n    outputLA = additionalL(outputL)\n    outputL = tf.keras.layers.Concatenate(axis=-1)([outputLT, outputLA])\n\n    # Passing right fused features through Transition Block 1\n    outputRT = transition_block(outputR)\n    outputRA = additionalL(outputR)\n    outputR = tf.keras.layers.Concatenate(axis=-1)([outputRT, outputRA])\n\n    # Combine left and right streams\n    output2 = tf.keras.layers.Concatenate(axis=-1)([outputL, outputR])\n\n    # Apply Transition Block 1 to ResNet50 final feature map\n    resnetT = transition_block(last_layer25)\n    resnetA = additionalL(last_layer25)\n    resnetout = tf.keras.layers.Concatenate(axis=-1)([resnetT, resnetA])\n\n    # Merge VGG19 final pooling layer with processed ResNet50 output\n    output1 = tf.keras.layers.Concatenate(axis=-1)([last_layer15, resnetout])\n\n    # Merge output1 and output2 to form ensemble feature representation\n    output_ensemble = tf.keras.layers.Concatenate(axis=-1)([output1, output2])\n\n    # Flatten ensemble features before dense layers\n    flatten = Flatten()(output_ensemble)\n    dense1 = Dense(256, activation='relu')(flatten)\n\n    # Dense layers for Vision Transformer output\n    vision_dense = Dense(256, activation='relu')(vision_output)\n\n    # Merge CNN dense features with Vision Transformer features\n    combined_dense = Concatenate()([dense1, vision_dense])\n\n    # Fully connected layers with dropout for regularization\n    combined_dense = Dropout(0.2)(combined_dense)\n    combined_dense = Dense(128, activation='relu')(combined_dense)\n    combined_dense = Dropout(0.3)(combined_dense)\n    combined_dense = Dense(64, activation='relu')(combined_dense)\n\n    # Final classification layer with 5 classes\n    model_output = Dense(5, activation='softmax', dtype='float32')(combined_dense)\n\n    # Build and compile the model\n    M = Model(inputs=input_tensor, outputs=model_output)\n    M.compile(optimizer=Adamax(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    return M\n","metadata":{"_cell_guid":"9fbde838-ac70-41ea-8f90-9209072eb999","_uuid":"3ebd9d04-5932-4b78-be0e-8dcbc29d10a7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###########  For batch size 128 ###########\ndata = tf.keras.utils.image_dataset_from_directory(directory = '/kaggle/input/cervical-preprocessed/Processed Dataset',\n                                                   color_mode = 'rgb',\n                                                   batch_size = 128,\n                                                   image_size = (128,128),\n                                                   shuffle=True,)","metadata":{"_cell_guid":"1a84f3df-18f7-4fea-985f-7df4bec98568","_uuid":"6293df88-e2fb-4f99-af7f-3ef1f2872c9c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = make_model()\n# #model.summary()","metadata":{"_cell_guid":"089987df-518a-4ae3-8f1f-7de32f4a44d3","_uuid":"9e2cebe3-7008-4436-8e1b-a39e4bf9bcee","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"leng = len(data)\nn_folds = 5  # number of folds here\n\ntrain_size = int(0.7 * leng)\ntest_size = int(0.3 * leng)\n\ntrain = data.take(train_size)\n\nremaining2 = data.skip(train_size)\ntest = remaining2.take(test_size)\n\n\n# calculate the size of each fold\nfold_size = train.cardinality().numpy() // n_folds\n\n# initialize lists to store the folds and remainders\nfolds = []\nremainders = [train]\n\n# loop over the number of folds and create each fold\nfor i in range(n_folds):\n    # take the first `fold_size` samples from the current remainder to create a fold\n    fold = remainders[-1].take(fold_size)\n    \n    # append the fold to the list of folds\n    folds.append(fold)\n    \n    # skip the samples in the fold to create the next remainder\n    remainder = remainders[-1].skip(fold_size)\n    \n    # append the remainder to the list of remainders\n    remainders.append(remainder)\n\n\n\n\n################## Test Itera ####################\n\ntest_iter = test.as_numpy_iterator()\n\ntest_set = {\"images\":np.empty((0,128,128,3)), \"labels\":np.empty(0)}\nwhile True:\n    try:\n        batch = test_iter.next()\n        test_set['images'] = np.concatenate((test_set['images'], batch[0]))\n        test_set['labels'] = np.concatenate((test_set['labels'], batch[1]))\n    except:\n        break\n\ny_true = test_set['labels']","metadata":{"_cell_guid":"83736f2e-b02d-496e-8304-b660e92181d2","_uuid":"e3ecd6d8-c612-4c03-adf4-fd7277a939e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# early_stop = EarlyStopping(\n#     monitor=\"val_accuracy\", \n#     patience=10,\n#     verbose=1,\n#     mode=\"max\",\n#     restore_best_weights=True, \n# )\n\n\n\nfor i in range(0,5):\n\n    model = make_model()\n    f1_scores = []\n    sn_scores = []\n    ppv_scores = []\n    test_accuracies = []\n    test_loss = []\n    kappa_scores = []\n    specificity_scores = []\n    print(\"Fold: \",i)\n    # create a list of folds to use as the training set\n    train_folds = [f for j, f in enumerate(folds) if j != i]\n\n    # concatenate the training folds into a single dataset\n    train_data = tf.data.experimental.sample_from_datasets(train_folds)\n\n    # train the model using the training data and the current fold as the validation data\n    history = model.fit(train_data, epochs = 40, validation_data=folds[i])\n    model.save(f\"/kaggle/working/model_fold_{i+1}.h5\")\n\n    # Evaluate the model on the test set\n    test_results = model.evaluate(test)\n\n    # Extract the evaluation metrics\n    test_loss_value = test_results[0]\n    test_accuracy = test_results[1]\n\n\n\n    # Perform necessary calculations for the evaluation metrics (e.g., F1-score, Sensitivity, PPV, Kappa)\n    y_pred = model.predict(test_set['images'])\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    f1_score1 = f1_score(y_true, y_pred_classes, average='macro')\n    sn = recall_score(y_true, y_pred_classes, average='macro')\n    ppv = precision_score(y_true, y_pred_classes, average='macro')\n    kappa = cohen_kappa_score(y_true, y_pred_classes)\n    conf_matrix = confusion_matrix(y_true, y_pred_classes)\n\n    # Extract TN (True Negatives) and FP (False Positives) from the confusion matrix\n    TN = conf_matrix[0, 0]\n    FP = conf_matrix[0, 1]\n\n    # Calculate specificity\n    specificity = TN / (TN + FP)\n\n    # Store the evaluation metrics for the current fold\n    f1_scores.append(f1_score1)\n    sn_scores.append(sn)\n    ppv_scores.append(ppv)\n    test_accuracies.append(test_accuracy)\n    test_loss.append(test_loss_value)\n    kappa_scores.append(kappa)\n    specificity_scores.append(specificity)\n\n\n\n    # Get the accuracy, loss, recall, and sensitivity for each epoch\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n\n    # Write the accuracy, loss, recall, and sensitivity to a CSV file\n    output_file_path = '/kaggle/working/accuracy_loss_recall_sensitivity.csv'\n\n    with open(output_file_path, mode='w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow(['Epoch', 'Train-Accuracy', 'Validation Accuracy', 'Train-Loss', 'Validation Loss', 'F1-Score', 'Sensitivity','Precition','Test-Acc','Test-Loss','Kappa','Specificity'])\n            for j in range(len(acc)):\n                writer.writerow([j+1, acc[j], val_acc[j], loss[j], val_loss[j], f1_scores, sn_scores,ppv_scores,test_accuracies,test_loss,kappa_scores,specificity_scores])\n\n    destination_file_path = '/kaggle/working/Bifusion_(-vit)_model_Fold ' + str(i+1) + '.csv'\n    shutil.copy(output_file_path, destination_file_path)","metadata":{"_cell_guid":"e0c14e40-d6a6-48d6-95c4-49cb35b9f196","_uuid":"1244c71c-485e-4712-8d67-81b2571aa1b1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Calculate FLOPs\n# flops = get_flops(model, batch_size=1)\n# print(f\"FLOPs: {flops / 10**9:.03} G\")","metadata":{"_cell_guid":"ae704672-555b-4732-a1de-dedefebc4550","_uuid":"eea28041-a572-4f0e-8020-eba016f18ddc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null}]}